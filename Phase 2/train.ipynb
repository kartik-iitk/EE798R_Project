{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Intelligent Pattern Recognition, EE798R\n",
    "\n",
    "##### *November 6th, 2024*\n",
    "\n",
    "# Project Phase 2\n",
    "\n",
    "**Kartik Anant Kulkarni (Section C, Roll no. 210493)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras.backend as K\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras.layers import Activation, Lambda\n",
    "from tensorflow.keras.layers import (\n",
    "    Conv1D,\n",
    "    SpatialDropout1D,\n",
    "    add,\n",
    "    GlobalAveragePooling1D,\n",
    ")\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.activations import sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow.keras.backend as K\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.optimizers import SGD, Adam\n",
    "from tensorflow.keras import callbacks\n",
    "from tensorflow.keras.layers import Layer, Dense, Input\n",
    "from tensorflow.keras.models import Model\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import copy\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import argparse\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model, Layer Definition and Helper Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Common Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Common_Model(object):\n",
    "    def __init__(self, save_path: str = \"\", name: str = \"Not Specified\"):\n",
    "        self.model = None\n",
    "        self.trained = False\n",
    "\n",
    "    def train(self, x_train, y_train, x_val, y_val):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def predict(self, samples):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def predict_proba(self, samples):\n",
    "        if not self.trained:\n",
    "            sys.stderr.write(\"No Model.\")\n",
    "            sys.exit(-1)\n",
    "        return self.model.predict_proba(samples)\n",
    "\n",
    "    def save_model(self, model_name: str):\n",
    "        raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Temporally Aware Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Temporal_Aware_Block(\n",
    "    x, s, i, activation, nb_filters, kernel_size, dropout_rate=0, name=\"\"\n",
    "):\n",
    "    original_x = x\n",
    "    # 1.1\n",
    "    conv_1_1 = Conv1D(\n",
    "        filters=nb_filters, kernel_size=kernel_size, dilation_rate=i, padding=\"causal\"\n",
    "    )(x)\n",
    "    conv_1_1 = BatchNormalization(trainable=True, axis=-1)(conv_1_1)\n",
    "    conv_1_1 = Activation(activation)(conv_1_1)\n",
    "    output_1_1 = SpatialDropout1D(dropout_rate)(conv_1_1)\n",
    "    # 2.1\n",
    "    conv_2_1 = Conv1D(\n",
    "        filters=nb_filters, kernel_size=kernel_size, dilation_rate=i, padding=\"causal\"\n",
    "    )(output_1_1)\n",
    "    conv_2_1 = BatchNormalization(trainable=True, axis=-1)(conv_2_1)\n",
    "    conv_2_1 = Activation(activation)(conv_2_1)\n",
    "    output_2_1 = SpatialDropout1D(dropout_rate)(conv_2_1)\n",
    "\n",
    "    if original_x.shape[-1] != output_2_1.shape[-1]:\n",
    "        original_x = Conv1D(filters=nb_filters, kernel_size=1, padding=\"same\")(\n",
    "            original_x\n",
    "        )\n",
    "\n",
    "    output_2_1 = Lambda(sigmoid)(output_2_1)\n",
    "    F_x = Lambda(lambda x: tf.multiply(x[0], x[1]))([original_x, output_2_1])\n",
    "    return F_x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TIMNET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TIMNET:\n",
    "    def __init__(\n",
    "        self,\n",
    "        nb_filters=64,\n",
    "        kernel_size=2,\n",
    "        nb_stacks=1,\n",
    "        dilations=None,\n",
    "        activation=\"relu\",\n",
    "        dropout_rate=0.1,\n",
    "        return_sequences=True,\n",
    "        name=\"TIMNET\",\n",
    "    ):\n",
    "        self.name = name\n",
    "        self.return_sequences = return_sequences\n",
    "        self.activation = activation\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.dilations = dilations\n",
    "        self.nb_stacks = nb_stacks\n",
    "        self.kernel_size = kernel_size\n",
    "        self.nb_filters = nb_filters\n",
    "\n",
    "        self.supports_masking = True\n",
    "        self.mask_value = 0.0\n",
    "\n",
    "        if not isinstance(nb_filters, int):\n",
    "            raise Exception(\"`nb_filters` should be an integer\")\n",
    "\n",
    "    def random_shuffle(self, inputs):\n",
    "        input_shape = tf.shape(inputs)\n",
    "        batch_size = input_shape[0]\n",
    "        seq_length = input_shape[1]\n",
    "        shuffle_indices = tf.random.shuffle(tf.range(seq_length))\n",
    "        shuffled_inputs = tf.gather(inputs, shuffle_indices, axis=1)\n",
    "        return shuffled_inputs\n",
    "\n",
    "    def __call__(self, inputs, mask=None):\n",
    "        if self.dilations is None:\n",
    "            self.dilations = 8\n",
    "\n",
    "        forward = inputs\n",
    "        backward = K.reverse(inputs, axes=1)\n",
    "\n",
    "        # Random shuffle transformation\n",
    "        shuffled = self.random_shuffle(inputs)\n",
    "\n",
    "        print(\"Input Shape=\", inputs.shape)\n",
    "\n",
    "        forward_convd = Conv1D(\n",
    "            filters=self.nb_filters, kernel_size=1, dilation_rate=1, padding=\"causal\"\n",
    "        )(forward)\n",
    "        backward_convd = Conv1D(\n",
    "            filters=self.nb_filters, kernel_size=1, dilation_rate=1, padding=\"causal\"\n",
    "        )(backward)\n",
    "        shuffled_convd = Conv1D(\n",
    "            filters=self.nb_filters, kernel_size=1, dilation_rate=1, padding=\"causal\"\n",
    "        )(shuffled)\n",
    "\n",
    "        final_skip_connection = []\n",
    "\n",
    "        # Initial skip connections\n",
    "        skip_out_forward = forward_convd\n",
    "        skip_out_backward = backward_convd\n",
    "        skip_out_shuffled = shuffled_convd\n",
    "\n",
    "        for s in range(self.nb_stacks):\n",
    "            for i in [2**i for i in range(self.dilations)]:\n",
    "                skip_out_forward = Temporal_Aware_Block(\n",
    "                    skip_out_forward,\n",
    "                    s,\n",
    "                    i,\n",
    "                    self.activation,\n",
    "                    self.nb_filters,\n",
    "                    self.kernel_size,\n",
    "                    self.dropout_rate + np.log2(i) * 0.0125,\n",
    "                    name=f\"{self.name}_stack{s}_dilation{i}_forward\",\n",
    "                )\n",
    "                skip_out_backward = Temporal_Aware_Block(\n",
    "                    skip_out_backward,\n",
    "                    s,\n",
    "                    i,\n",
    "                    self.activation,\n",
    "                    self.nb_filters,\n",
    "                    self.kernel_size,\n",
    "                    self.dropout_rate + np.log2(i) * 0.0125,\n",
    "                    name=f\"{self.name}_stack{s}_dilation{i}_backward\",\n",
    "                )\n",
    "                skip_out_shuffled = Temporal_Aware_Block(\n",
    "                    skip_out_shuffled,\n",
    "                    s,\n",
    "                    i,\n",
    "                    self.activation,\n",
    "                    self.nb_filters,\n",
    "                    self.kernel_size,\n",
    "                    self.dropout_rate + np.log2(i) * 0.0125,\n",
    "                    name=f\"{self.name}_stack{s}_dilation{i}_shuffled\",\n",
    "                )\n",
    "\n",
    "                # Combine forward, backward, and shuffled skip connections\n",
    "                temp_skip = add(\n",
    "                    [skip_out_forward, skip_out_backward, skip_out_shuffled],\n",
    "                    name=\"biadd_\" + str(i) + str(s),\n",
    "                )\n",
    "                temp_skip = GlobalAveragePooling1D()(temp_skip)\n",
    "                temp_skip = tf.expand_dims(temp_skip, axis=1)\n",
    "                final_skip_connection.append(temp_skip)\n",
    "\n",
    "        # Concatenate skip connections\n",
    "        output_2 = final_skip_connection[0]\n",
    "        for i, item in enumerate(final_skip_connection):\n",
    "            if i == 0:\n",
    "                continue\n",
    "            output_2 = K.concatenate([output_2, item], axis=-2)\n",
    "\n",
    "        x = output_2\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dynamic Fusion Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeightLayer(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(WeightLayer, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.kernel = self.add_weight(\n",
    "            name=\"kernel\",\n",
    "            shape=(input_shape[1], 1),\n",
    "            initializer=\"uniform\",\n",
    "            trainable=True,\n",
    "        )\n",
    "        super(WeightLayer, self).build(input_shape)\n",
    "\n",
    "    def call(self, x):\n",
    "        tempx = tf.transpose(x, [0, 2, 1])\n",
    "        x = K.dot(tempx, self.kernel)\n",
    "        x = tf.squeeze(x, axis=-1)\n",
    "        return x\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], input_shape[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(Layer):\n",
    "    def __init__(self, output_dim):\n",
    "        super(Attention, self).__init__()\n",
    "        self.output_dim = output_dim\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        # Initialize a learnable query vector\n",
    "        self.query = self.add_weight(\n",
    "            shape=(1, input_shape[-1]), initializer=\"random_normal\", trainable=True\n",
    "        )\n",
    "\n",
    "    def call(self, values):\n",
    "        # Dot product attention between the query and the values (keys and values are same here)\n",
    "        scores = tf.matmul(self.query, values, transpose_b=True) / tf.sqrt(\n",
    "            tf.cast(self.output_dim, tf.float32)\n",
    "        )\n",
    "        attention_weights = tf.nn.softmax(scores, axis=-1)\n",
    "        return tf.matmul(attention_weights, values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Checkpointing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss_list = []\n",
    "val_loss_list = []\n",
    "val_acc_list = []\n",
    "training_acc_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomModelCheckpoint(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, weight_path, save_freq=5, save_best_only=False):\n",
    "        super(CustomModelCheckpoint, self).__init__()\n",
    "        self.weight_path = weight_path\n",
    "        self.save_freq = save_freq\n",
    "        self.save_best_only = save_best_only\n",
    "        self.best_weights = None\n",
    "        self.best_val_loss = float(\"inf\")\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        # Save every `self.save_freq` epochs\n",
    "        train_loss_list.append((epoch, logs.get(\"loss\")))\n",
    "        val_loss_list.append((epoch, logs.get(\"val_loss\")))\n",
    "        val_acc_list.append((epoch, logs.get(\"val_accuracy\")))\n",
    "        training_acc_list.append((epoch, logs.get(\"accuracy\")))\n",
    "\n",
    "        if (epoch + 1) % self.save_freq == 0:\n",
    "            if self.save_best_only:\n",
    "                val_loss = logs.get(\"val_loss\")\n",
    "                if val_loss is not None and val_loss < self.best_val_loss:\n",
    "                    self.best_val_loss = val_loss\n",
    "                    print(\n",
    "                        f\"\\nEpoch {epoch + 1}: val_loss improved to {self.best_val_loss}, saving model\"\n",
    "                    )\n",
    "                    self.model.save_weights(self.weight_path)\n",
    "                    self.best_weights = self.model.get_weights()  # Store best weights\n",
    "                else:\n",
    "                    print(\n",
    "                        f\"\\nEpoch {epoch + 1}: val_loss did not improve, skipping save\"\n",
    "                    )\n",
    "            else:\n",
    "                print(f\"\\nEpoch {epoch + 1}: saving model\")\n",
    "                self.model.save_weights(self.weight_path)\n",
    "        else:\n",
    "            print(f\"\\nEpoch {epoch + 1}: No save this epoch\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TIMNET_Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smooth_labels(labels, factor=0.1):\n",
    "    labels *= 1 - factor\n",
    "    labels += factor / labels.shape[1]\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x, axis=-1):\n",
    "    ex = K.exp(x - K.max(x, axis=axis, keepdims=True))\n",
    "    return ex / K.sum(ex, axis=axis, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def focal_loss(y_true, y_pred):\n",
    "    alpha = 0.25  # A balancing factor to adjust the loss contribution of each class.\n",
    "    gamma = 2.0  # Focusing parameter to down-weight easy examples.\n",
    "    label_smoothing = 0.1  # Amount of smoothing to apply to labels.\n",
    "\n",
    "    num_classes = tf.cast(tf.shape(y_true)[-1], tf.float32)\n",
    "    y_true = (1.0 - label_smoothing) * y_true + (label_smoothing / num_classes)\n",
    "\n",
    "    ce_loss = tf.keras.losses.categorical_crossentropy(y_true, y_pred)\n",
    "    p_t = tf.reduce_sum(y_true * y_pred, axis=-1)\n",
    "    focal_factor = alpha * tf.pow(1 - p_t, gamma)\n",
    "\n",
    "    focal_loss = focal_factor * ce_loss\n",
    "    return tf.reduce_mean(focal_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TIMNET_Model(Common_Model):\n",
    "    def __init__(self, args, input_shape, class_label, **params):\n",
    "        super(TIMNET_Model, self).__init__(**params)\n",
    "        self.args = args\n",
    "        self.data_shape = input_shape\n",
    "        self.num_classes = len(class_label)\n",
    "        self.class_label = class_label\n",
    "        self.matrix = []\n",
    "        self.eva_matrix = []\n",
    "        self.acc = 0\n",
    "        print(\"TIMNET MODEL SHAPE:\", input_shape)\n",
    "\n",
    "    def create_model(self):\n",
    "        self.inputs = Input(shape=(self.data_shape[0], self.data_shape[1]))\n",
    "\n",
    "        self.mfcc_drop = SpatialDropout1D(self.args.temporal_drop)(self.inputs)\n",
    "\n",
    "        self.multi_decision = TIMNET(\n",
    "            nb_filters=self.args.filter_size,\n",
    "            kernel_size=self.args.kernel_size,\n",
    "            nb_stacks=self.args.stack_size,\n",
    "            dilations=self.args.dilation_size,\n",
    "            dropout_rate=self.args.dropout,\n",
    "            activation=self.args.activation,\n",
    "            return_sequences=True,\n",
    "            name=\"TIMNET\",\n",
    "        )(self.mfcc_drop)\n",
    "\n",
    "        # Attention Layer\n",
    "        self.attention_pooling = Attention(output_dim=self.multi_decision.shape[-1])\n",
    "        self.decision = self.attention_pooling(self.multi_decision)\n",
    "        self.decision = tf.squeeze(self.decision, axis=1)\n",
    "\n",
    "        # Uncomment Below for the Weighted Layer\n",
    "        # self.decision = WeightLayer()(self.multi_decision)\n",
    "\n",
    "        self.hidden_prediction = Dense(128, activation=\"relu\")(self.decision)\n",
    "        self.predictions = Dense(self.num_classes, activation=\"softmax\")(\n",
    "            self.hidden_prediction  # self.decision\n",
    "        )\n",
    "        self.model = Model(inputs=self.inputs, outputs=self.predictions)\n",
    "\n",
    "        self.model.compile(\n",
    "            loss=focal_loss,\n",
    "            # loss=\"categorical_crossentropy\",\n",
    "            optimizer=Adam(\n",
    "                learning_rate=self.args.lr,\n",
    "                beta_1=self.args.beta1,\n",
    "                beta_2=self.args.beta2,\n",
    "                epsilon=1e-8,\n",
    "            ),\n",
    "            metrics=[\"accuracy\"],\n",
    "        )\n",
    "        print(\"Temporal create success!\")\n",
    "\n",
    "    def train(self, x, y):\n",
    "        train_loss_list = []\n",
    "        val_loss_list = []\n",
    "        val_acc_list = []\n",
    "        training_acc_list = []\n",
    "\n",
    "        filepath = self.args.model_path\n",
    "        resultpath = self.args.result_path\n",
    "\n",
    "        if not os.path.exists(filepath):\n",
    "            os.mkdir(filepath)\n",
    "        if not os.path.exists(resultpath):\n",
    "            os.mkdir(resultpath)\n",
    "\n",
    "        i = 1\n",
    "        now = datetime.datetime.now()\n",
    "        now_time = datetime.datetime.strftime(now, \"%Y-%m-%d_%H-%M-%S\")\n",
    "        kfold = KFold(\n",
    "            n_splits=self.args.split_fold,\n",
    "            shuffle=True,\n",
    "            random_state=self.args.random_seed,\n",
    "        )\n",
    "        avg_accuracy = 0\n",
    "        avg_loss = 0\n",
    "        for train, test in kfold.split(x, y):\n",
    "            self.create_model()\n",
    "            y_train = smooth_labels(copy.deepcopy(y[train]), 0.1)\n",
    "            folder_address = (\n",
    "                filepath\n",
    "                + self.args.data\n",
    "                + \"_\"\n",
    "                + str(self.args.random_seed)\n",
    "                + \"_\"\n",
    "                + now_time\n",
    "            )\n",
    "            if not os.path.exists(folder_address):\n",
    "                os.mkdir(folder_address)\n",
    "            weight_path = (\n",
    "                folder_address\n",
    "                + \"/\"\n",
    "                + str(self.args.split_fold)\n",
    "                + \"-fold_weights_best_\"\n",
    "                + str(i)\n",
    "                + \".hdf5\"\n",
    "            )\n",
    "            # checkpoint = callbacks.ModelCheckpoint(\n",
    "            #     weight_path, verbose=1, save_weights_only=True, save_best_only=True\n",
    "            # )\n",
    "            checkpoint = CustomModelCheckpoint(\n",
    "                weight_path=weight_path, save_freq=1, save_best_only=True\n",
    "            )\n",
    "            max_acc = 0\n",
    "            best_eva_list = []\n",
    "            h = self.model.fit(\n",
    "                x[train],\n",
    "                y_train,\n",
    "                validation_data=(x[test], y[test]),\n",
    "                batch_size=self.args.batch_size,\n",
    "                epochs=self.args.epoch,\n",
    "                verbose=1,\n",
    "                callbacks=[checkpoint],\n",
    "            )\n",
    "            self.model.load_weights(weight_path)\n",
    "            best_eva_list = self.model.evaluate(x[test], y[test])\n",
    "            avg_loss += best_eva_list[0]\n",
    "            avg_accuracy += best_eva_list[1]\n",
    "            print(\n",
    "                str(i) + \"_Model evaluation: \",\n",
    "                best_eva_list,\n",
    "                \"   Now ACC:\",\n",
    "                str(round(avg_accuracy * 10000) / 100 / i),\n",
    "            )\n",
    "            i += 1\n",
    "            y_pred_best = self.model.predict(x[test])\n",
    "            self.matrix.append(\n",
    "                confusion_matrix(\n",
    "                    np.argmax(y[test], axis=1), np.argmax(y_pred_best, axis=1)\n",
    "                )\n",
    "            )\n",
    "            em = classification_report(\n",
    "                np.argmax(y[test], axis=1),\n",
    "                np.argmax(y_pred_best, axis=1),\n",
    "                target_names=self.class_label,\n",
    "                output_dict=True,\n",
    "            )\n",
    "            self.eva_matrix.append(em)\n",
    "            print(\n",
    "                classification_report(\n",
    "                    np.argmax(y[test], axis=1),\n",
    "                    np.argmax(y_pred_best, axis=1),\n",
    "                    target_names=self.class_label,\n",
    "                )\n",
    "            )\n",
    "\n",
    "        print(\"Average ACC:\", avg_accuracy / self.args.split_fold)\n",
    "        self.acc = avg_accuracy / self.args.split_fold\n",
    "        writer = pd.ExcelWriter(\n",
    "            resultpath\n",
    "            + self.args.data\n",
    "            + \"_\"\n",
    "            + str(self.args.split_fold)\n",
    "            + \"fold_\"\n",
    "            + str(round(self.acc * 10000) / 100)\n",
    "            + \"_\"\n",
    "            + str(self.args.random_seed)\n",
    "            + \"_\"\n",
    "            + now_time\n",
    "            + \".xlsx\"\n",
    "        )\n",
    "        for i, item in enumerate(self.matrix):\n",
    "            temp = {}\n",
    "            temp[\" \"] = self.class_label\n",
    "            for j, l in enumerate(item):\n",
    "                temp[self.class_label[j]] = item[j]\n",
    "            data1 = pd.DataFrame(temp)\n",
    "            data1.to_excel(writer, sheet_name=str(i))  # ), encoding=\"utf8\")\n",
    "\n",
    "            df = pd.DataFrame(self.eva_matrix[i]).transpose()\n",
    "            df.to_excel(writer, sheet_name=str(i) + \"_evaluate\")  # , encoding='utf8')\n",
    "        # writer.save()\n",
    "        writer.close()\n",
    "\n",
    "        K.clear_session()\n",
    "        self.matrix = []\n",
    "        self.eva_matrix = []\n",
    "        self.acc = 0\n",
    "        self.trained = True\n",
    "\n",
    "    def test(self, x, y, path):\n",
    "        i = 1\n",
    "        kfold = KFold(\n",
    "            n_splits=self.args.split_fold,\n",
    "            shuffle=True,\n",
    "            random_state=self.args.random_seed,\n",
    "        )\n",
    "        avg_accuracy = 0\n",
    "        avg_loss = 0\n",
    "        x_feats = []\n",
    "        y_labels = []\n",
    "        for train, test in kfold.split(x, y):\n",
    "            self.create_model()\n",
    "            weight_path = (\n",
    "                path\n",
    "                + \"/\"\n",
    "                + str(self.args.split_fold)\n",
    "                + \"-fold_weights_best_\"\n",
    "                + str(i)\n",
    "                + \".hdf5\"\n",
    "            )\n",
    "            self.model.fit(\n",
    "                x[train],\n",
    "                y[train],\n",
    "                validation_data=(x[test], y[test]),\n",
    "                batch_size=64,\n",
    "                epochs=0,\n",
    "                verbose=0,\n",
    "            )\n",
    "            self.model.load_weights(weight_path)  # +source_name+'_single_best.hdf5')\n",
    "            best_eva_list = self.model.evaluate(x[test], y[test])\n",
    "            avg_loss += best_eva_list[0]\n",
    "            avg_accuracy += best_eva_list[1]\n",
    "            print(\n",
    "                str(i) + \"_Model evaluation: \",\n",
    "                best_eva_list,\n",
    "                \"   Now ACC:\",\n",
    "                str(round(avg_accuracy * 10000) / 100 / i),\n",
    "            )\n",
    "            i += 1\n",
    "            y_pred_best = self.model.predict(x[test])\n",
    "            self.matrix.append(\n",
    "                confusion_matrix(\n",
    "                    np.argmax(y[test], axis=1), np.argmax(y_pred_best, axis=1)\n",
    "                )\n",
    "            )\n",
    "            em = classification_report(\n",
    "                np.argmax(y[test], axis=1),\n",
    "                np.argmax(y_pred_best, axis=1),\n",
    "                target_names=self.class_label,\n",
    "                output_dict=True,\n",
    "            )\n",
    "            self.eva_matrix.append(em)\n",
    "            print(\n",
    "                classification_report(\n",
    "                    np.argmax(y[test], axis=1),\n",
    "                    np.argmax(y_pred_best, axis=1),\n",
    "                    target_names=self.class_label,\n",
    "                )\n",
    "            )\n",
    "            caps_layer_model = Model(\n",
    "                inputs=self.model.input, outputs=self.model.get_layer(index=-2).output\n",
    "            )\n",
    "            feature_source = caps_layer_model.predict(x[test])\n",
    "            x_feats.append(feature_source)\n",
    "            y_labels.append(y[test])\n",
    "        print(\"Average ACC:\", avg_accuracy / self.args.split_fold)\n",
    "        self.acc = avg_accuracy / self.args.split_fold\n",
    "        return x_feats, y_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Hyperparameters\n",
    "\n",
    "Put all the processed MFCCs in the MFCC folder. Replace the following hyperparameters based on the chosen dataset using the table below to obtain the quoted results. The other hyperparameters can be left as it is based on their default value.\n",
    "\n",
    "| data | beta1 | beta2 | batch_size | epoch | dropout | temporal_drop | split_fold |\n",
    "| ----- | ----- | ----- | ----- | ----- | ----- | ----- | ----- |\n",
    "| SAVEE | 0.875 | 0.98 | 256 | 500 | 0.05 | 0.05 | 8 |\n",
    "| EMODB | 0.875 | 0.98 | 256 | 400 | 0.05 | 0.05 | 10 |\n",
    "\n",
    "**Note**: It is observed that the above hyperparameters are mostly same for all datasets. However, they can be tuned specifically using the values mentioned in Phase 1. However, the dropout methodology being updated, that parameter requires special consideration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "\n",
    "parser.add_argument(\"--mode\", type=str, default=\"train\")\n",
    "parser.add_argument(\"--model_path\", type=str, default=\"./Models/\")\n",
    "parser.add_argument(\"--result_path\", type=str, default=\"./Results/\")\n",
    "parser.add_argument(\"--test_path\", type=str, default=\"./Models/SAVEE\")\n",
    "parser.add_argument(\"--data\", type=str, default=\"SAVEE\")\n",
    "parser.add_argument(\"--lr\", type=float, default=0.001)\n",
    "parser.add_argument(\"--beta1\", type=float, default=0.875)\n",
    "parser.add_argument(\"--beta2\", type=float, default=0.98)\n",
    "parser.add_argument(\"--batch_size\", type=int, default=256)\n",
    "parser.add_argument(\"--epoch\", type=int, default=500)\n",
    "parser.add_argument(\"--dropout\", type=float, default=0.05)\n",
    "parser.add_argument(\"--random_seed\", type=int, default=46)\n",
    "parser.add_argument(\"--activation\", type=str, default=\"relu\")\n",
    "# parser.add_argument(\"--filter_size\", type=int, default=39)\n",
    "parser.add_argument(\n",
    "    \"--dilation_size\", type=int, default=8\n",
    ")  # If you want to train model on IEMOCAP, you should modify this parameter to 10 due to the long duration of speech signals.\n",
    "parser.add_argument(\"--kernel_size\", type=int, default=2)\n",
    "parser.add_argument(\"--stack_size\", type=int, default=1)\n",
    "parser.add_argument(\"--split_fold\", type=int, default=8)\n",
    "parser.add_argument(\"--gpu\", type=str, default=\"0\")\n",
    "parser.add_argument(\"--temporal_drop\", type=float, default=0.05)\n",
    "\n",
    "# args = parser.parse_args()\n",
    "args, _ = parser.parse_known_args()\n",
    "\n",
    "# Check if filter_size is throwing an issue when added\n",
    "try:\n",
    "    if hasattr(args, \"filter_size\"):\n",
    "        filter_size = args.filter_size\n",
    "    else:\n",
    "        args.filter_size = 39\n",
    "except Exception as e:\n",
    "    print(f\"Error with filter_size: {e}\")\n",
    "print(f\"Filter Size: {args.filter_size}\")\n",
    "\n",
    "if args.data == \"IEMOCAP\" and args.dilation_size != 10:\n",
    "    args.dilation_size = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = args.gpu\n",
    "gpus = tf.config.experimental.list_physical_devices(device_type=\"GPU\")\n",
    "config = tf.compat.v1.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "session = tf.compat.v1.Session(config=config)\n",
    "print(f\"###gpus:{gpus}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# List all physical devices recognized by TensorFlow\n",
    "physical_devices = tf.config.list_physical_devices()\n",
    "print(\"Physical devices detected by TensorFlow:\")\n",
    "for device in physical_devices:\n",
    "    print(device)\n",
    "\n",
    "# Check specifically for GPU devices\n",
    "gpu_devices = tf.config.list_physical_devices(\"GPU\")\n",
    "if gpu_devices:\n",
    "    print(\"\\nGPU device(s) available:\")\n",
    "    for gpu in gpu_devices:\n",
    "        print(gpu)\n",
    "else:\n",
    "    print(\"\\nNo GPU devices found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLASS_LABELS_finetune = (\"angry\", \"fear\", \"happy\", \"neutral\", \"sad\")\n",
    "CASIA_CLASS_LABELS = (\"angry\", \"fear\", \"happy\", \"neutral\", \"sad\", \"surprise\")  # CASIA\n",
    "EMODB_CLASS_LABELS = (\n",
    "    \"angry\",\n",
    "    \"boredom\",\n",
    "    \"disgust\",\n",
    "    \"fear\",\n",
    "    \"happy\",\n",
    "    \"neutral\",\n",
    "    \"sad\",\n",
    ")  # EMODB\n",
    "SAVEE_CLASS_LABELS = (\n",
    "    \"angry\",\n",
    "    \"disgust\",\n",
    "    \"fear\",\n",
    "    \"happy\",\n",
    "    \"neutral\",\n",
    "    \"sad\",\n",
    "    \"surprise\",\n",
    ")  # SAVEE\n",
    "RAVDE_CLASS_LABELS = (\n",
    "    \"angry\",\n",
    "    \"calm\",\n",
    "    \"disgust\",\n",
    "    \"fear\",\n",
    "    \"happy\",\n",
    "    \"neutral\",\n",
    "    \"sad\",\n",
    "    \"surprise\",\n",
    ")  # rav\n",
    "IEMOCAP_CLASS_LABELS = (\"angry\", \"happy\", \"neutral\", \"sad\")  # iemocap\n",
    "EMOVO_CLASS_LABELS = (\n",
    "    \"angry\",\n",
    "    \"disgust\",\n",
    "    \"fear\",\n",
    "    \"happy\",\n",
    "    \"neutral\",\n",
    "    \"sad\",\n",
    "    \"surprise\",\n",
    ")  # emovo\n",
    "CLASS_LABELS_dict = {\n",
    "    \"CASIA\": CASIA_CLASS_LABELS,\n",
    "    \"EMODB\": EMODB_CLASS_LABELS,\n",
    "    \"EMOVO\": EMOVO_CLASS_LABELS,\n",
    "    \"IEMOCAP\": IEMOCAP_CLASS_LABELS,\n",
    "    \"RAVDE\": RAVDE_CLASS_LABELS,\n",
    "    \"SAVEE\": SAVEE_CLASS_LABELS,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.load(\"./MFCC/\" + args.data + \".npy\", allow_pickle=True).item()\n",
    "x_source = data[\"x\"]\n",
    "y_source = data[\"y\"]\n",
    "CLASS_LABELS = CLASS_LABELS_dict[args.data]\n",
    "\n",
    "\n",
    "model = TIMNET_Model(\n",
    "    args=args, input_shape=x_source.shape[1:], class_label=CLASS_LABELS\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.mode == \"train\":\n",
    "    model.train(x_source, y_source)\n",
    "elif args.mode == \"test\":\n",
    "    x_feats, y_labels = model.test(\n",
    "        x_source, y_source, path=args.test_path\n",
    "    )  # x_feats and y_labels are test datas for t-sne"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Accuracy and Losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_loss = pd.DataFrame(train_loss_list, columns=[\"epoch\", \"train_loss\"])\n",
    "df_val_loss = pd.DataFrame(val_loss_list, columns=[\"epoch\", \"val_loss\"])\n",
    "df_val_acc = pd.DataFrame(val_acc_list, columns=[\"epoch\", \"val_acc\"])\n",
    "df_training_acc = pd.DataFrame(training_acc_list, columns=[\"epoch\", \"training_acc\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_loss = df_train_loss.groupby(\"epoch\").mean().reset_index()\n",
    "df_val_loss = df_val_loss.groupby(\"epoch\").mean().reset_index()\n",
    "df_val_acc = df_val_acc.groupby(\"epoch\").mean().reset_index()\n",
    "df_training_acc = df_training_acc.groupby(\"epoch\").mean().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5, 3), dpi=200)\n",
    "plt.plot(df_train_loss[\"epoch\"], df_train_loss[\"train_loss\"], label=\"train_loss\")\n",
    "plt.plot(df_val_loss[\"epoch\"], df_val_loss[\"val_loss\"], label=\"val_loss\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(args.data + \" Avg. Loss across Splits\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"./logs/\" + args.data + \"_loss.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5, 3), dpi=200)\n",
    "plt.plot(\n",
    "    df_training_acc[\"epoch\"], df_training_acc[\"training_acc\"], label=\"training_acc\"\n",
    ")\n",
    "plt.plot(df_val_acc[\"epoch\"], df_val_acc[\"val_acc\"], label=\"val_acc\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(args.data + \" Avg. Accuracy across Splits\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"./logs/\" + args.data + \"_acc.png\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "IPR_P1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
